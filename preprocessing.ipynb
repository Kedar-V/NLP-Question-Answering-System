{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of SciQproject.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5JbDpKwi6vQ",
        "outputId": "8267b044-7170-4fad-a50f-1af30e1efc14"
      },
      "source": [
        "!wget https://ai2-public-datasets.s3.amazonaws.com/sciq/SciQ.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-13 19:38:05--  https://ai2-public-datasets.s3.amazonaws.com/sciq/SciQ.zip\n",
            "Resolving ai2-public-datasets.s3.amazonaws.com (ai2-public-datasets.s3.amazonaws.com)... 52.218.196.114\n",
            "Connecting to ai2-public-datasets.s3.amazonaws.com (ai2-public-datasets.s3.amazonaws.com)|52.218.196.114|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2821345 (2.7M) [binary/octet-stream]\n",
            "Saving to: ‘SciQ.zip’\n",
            "\n",
            "SciQ.zip            100%[===================>]   2.69M  4.52MB/s    in 0.6s    \n",
            "\n",
            "2021-10-13 19:38:06 (4.52 MB/s) - ‘SciQ.zip’ saved [2821345/2821345]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLDuRo3wWuTp",
        "outputId": "832a1de4-cfc1-48be-f5c5-f999dc4ee5e4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dt0pd7Ztjm3C",
        "outputId": "ff02b9a9-d262-4f81-ea87-f9aa97e05421"
      },
      "source": [
        "!unzip /content/SciQ.zip "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/SciQ.zip\n",
            "   creating: SciQ dataset-2 3/\n",
            "  inflating: SciQ dataset-2 3/license.txt  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/SciQ dataset-2 3/\n",
            "  inflating: __MACOSX/SciQ dataset-2 3/._license.txt  \n",
            "  inflating: SciQ dataset-2 3/readme.txt  \n",
            "  inflating: __MACOSX/SciQ dataset-2 3/._readme.txt  \n",
            "  inflating: SciQ dataset-2 3/test.json  \n",
            "  inflating: __MACOSX/SciQ dataset-2 3/._test.json  \n",
            "  inflating: SciQ dataset-2 3/train.json  \n",
            "  inflating: __MACOSX/SciQ dataset-2 3/._train.json  \n",
            "  inflating: SciQ dataset-2 3/valid.json  \n",
            "  inflating: __MACOSX/SciQ dataset-2 3/._valid.json  \n",
            "  inflating: __MACOSX/._SciQ dataset-2 3  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg1uwjOv69fN"
      },
      "source": [
        "import os\n",
        "os.mkdir('/content/train_para/')\n",
        "os.mkdir('/content/train_qa/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ndu-sMRXbZ97"
      },
      "source": [
        "1.   Replace numbers and dashes/special characters with ''\n",
        "2.   Remove lines with Figure\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thwHrXALjGbB",
        "outputId": "f2cb7469-ba65-4a50-c361-b7ccf37a3fd5"
      },
      "source": [
        "WORKING_DIR = '/content/'\n",
        "\n",
        "import json \n",
        "\n",
        "def remove_space(x):\n",
        "\treturn x.lstrip(' ')\n",
        "\n",
        "def get_sentences(p):\n",
        "  nlp = spacy.load('en')\n",
        "  tokens = nlp(p)\n",
        "  return [sent.string.strip() for sent in tokens]\n",
        "\n",
        "import re\n",
        "alphabets= \"([A-Za-z])\"\n",
        "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
        "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
        "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
        "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
        "websites = \"[.](com|net|org|io|gov)\"\n",
        "digits = \"([0-9])\"\n",
        "\n",
        "def split_into_sentences(text):\n",
        "    text = \" \" + text + \"  \"\n",
        "    text = text.replace(\"\\n\",\" \")\n",
        "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
        "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
        "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
        "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
        "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
        "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
        "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
        "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
        "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
        "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
        "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
        "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
        "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
        "    text = text.replace(\".\",\".<stop>\")\n",
        "    text = text.replace(\"?\",\"?<stop>\")\n",
        "    text = text.replace(\"!\",\"!<stop>\")\n",
        "    text = text.replace(\"<prd>\",\".\")\n",
        "    sentences = text.split(\"<stop>\")\n",
        "    sentences = sentences[:-1]\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    return sentences\n",
        "\n",
        "def add_newline(x):\n",
        "  return x + '\\n'\n",
        "\n",
        "def clean_text(p):\n",
        "  numbers = \"[0-9]\"\n",
        "  special = \"\\W+\"\n",
        "  p = re.sub(numbers, \" \", p)\n",
        "  p = re.sub(special, \" \", p)\n",
        "\n",
        "  return p\n",
        "\n",
        "\n",
        "with open('/content/SciQ dataset-2 3/train.json') as f :\n",
        "    data = json.load(f)\n",
        "    #print(data[0])\n",
        "    count = 0\n",
        "    full_set = []\n",
        "    for para in data :\n",
        "        # support = para['support'].replace('.','\\n.').split('.')\n",
        "        print(para['support'])\n",
        "        support = split_into_sentences(para['support'])\n",
        "\n",
        "        for i in support:\n",
        "          if 'figure' in i.lower():\n",
        "            continue\n",
        "        \n",
        "        support = [clean_text(i) for i in support]\n",
        "        support = list(map(add_newline, support))\n",
        "        with open(f'/content/train_para/{count}.txt', 'w') as s :\n",
        "            s.writelines(support)\n",
        "        count += 1\n",
        "        full_set = full_set + support\n",
        "        break\n",
        "    print(full_set[0:5])\n",
        "\n",
        "kb = ''.join(full_set)\n",
        "# print(len(kb))\n",
        "with open('/content/train_qa/kb.txt', 'w') as f:\n",
        "  f.write(kb)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mesophiles grow best in moderate temperature, typically between 25°C and 40°C (77°F and 104°F). Mesophiles are often found living in or on the bodies of humans or other animals. The optimal growth temperature of many pathogenic mesophiles is 37°C (98°F), the normal human body temperature. Mesophilic organisms have important uses in food preparation, including cheese, yogurt, beer and wine.\n",
            "['Mesophiles grow best in moderate temperature typically between C and C F and F \\n', 'Mesophiles are often found living in or on the bodies of humans or other animals \\n', 'The optimal growth temperature of many pathogenic mesophiles is C F the normal human body temperature \\n', 'Mesophilic organisms have important uses in food preparation including cheese yogurt beer and wine \\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wmTUWcVfide"
      },
      "source": [
        "text = 'Figure below shows an example of each type of fatty acid'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGETq4nFkrAJ",
        "outputId": "1a7fe587-48eb-4190-f7d9-b44b7fff06b4"
      },
      "source": [
        "print(len(full_set),count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BjKo1T5L35v",
        "outputId": "c0f1cc8c-193e-4b35-b48e-e6fa6f793cee"
      },
      "source": [
        "df = \n",
        "\n",
        "with open('/content/SciQ dataset-2 3/train.json') as f :\n",
        "    data = json.load(f)\n",
        "    #print(data[0])\n",
        "    count = 0\n",
        "    qanda = []\n",
        "    for para in data :\n",
        "        print(para)\n",
        "        count += 1\n",
        "        if count == 13:\n",
        "          break\n",
        "        \n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': 'What type of organism is commonly used in preparation of foods such as cheese and yogurt?', 'distractor3': 'viruses', 'distractor1': 'protozoa', 'distractor2': 'gymnosperms', 'correct_answer': 'mesophilic organisms', 'support': 'Mesophiles grow best in moderate temperature, typically between 25°C and 40°C (77°F and 104°F). Mesophiles are often found living in or on the bodies of humans or other animals. The optimal growth temperature of many pathogenic mesophiles is 37°C (98°F), the normal human body temperature. Mesophilic organisms have important uses in food preparation, including cheese, yogurt, beer and wine.'}\n",
            "{'question': 'What phenomenon makes global winds blow northeast to southwest or the reverse in the northern hemisphere and northwest to southeast or the reverse in the southern hemisphere?', 'distractor3': 'tropical effect', 'distractor1': 'muon effect', 'distractor2': 'centrifugal effect', 'correct_answer': 'coriolis effect', 'support': 'Without Coriolis Effect the global winds would blow north to south or south to north. But Coriolis makes them blow northeast to southwest or the reverse in the Northern Hemisphere. The winds blow northwest to southeast or the reverse in the southern hemisphere.'}\n",
            "{'question': 'Changes from a less-ordered state to a more-ordered state (such as a liquid to a solid) are always what?', 'distractor3': 'endothermic', 'distractor1': 'unbalanced', 'distractor2': 'reactive', 'correct_answer': 'exothermic', 'support': 'Summary Changes of state are examples of phase changes, or phase transitions. All phase changes are accompanied by changes in the energy of a system. Changes from a more-ordered state to a less-ordered state (such as a liquid to a gas) areendothermic. Changes from a less-ordered state to a more-ordered state (such as a liquid to a solid) are always exothermic. The conversion of a solid to a liquid is called fusion (or melting). The energy required to melt 1 mol of a substance is its enthalpy of fusion (ΔHfus). The energy change required to vaporize 1 mol of a substance is the enthalpy of vaporization (ΔHvap). The direct conversion of a solid to a gas is sublimation. The amount of energy needed to sublime 1 mol of a substance is its enthalpy of sublimation (ΔHsub) and is the sum of the enthalpies of fusion and vaporization. Plots of the temperature of a substance versus heat added or versus heating time at a constant rate of heating are calledheating curves. Heating curves relate temperature changes to phase transitions. A superheated liquid, a liquid at a temperature and pressure at which it should be a gas, is not stable. A cooling curve is not exactly the reverse of the heating curve because many liquids do not freeze at the expected temperature. Instead, they form a supercooled liquid, a metastable liquid phase that exists below the normal melting point. Supercooled liquids usually crystallize on standing, or adding a seed crystal of the same or another substance can induce crystallization.'}\n",
            "{'question': 'What is the least dangerous radioactive decay?', 'distractor3': 'zeta decay', 'distractor1': 'beta decay', 'distractor2': 'gamma decay', 'correct_answer': 'alpha decay', 'support': 'All radioactive decay is dangerous to living things, but alpha decay is the least dangerous.'}\n",
            "{'question': 'Kilauea in hawaii is the world’s most continuously active volcano. very active volcanoes characteristically eject red-hot rocks and lava rather than this?', 'distractor3': 'magma', 'distractor1': 'greenhouse gases', 'distractor2': 'carbon and smog', 'correct_answer': 'smoke and ash', 'support': 'Example 3.5 Calculating Projectile Motion: Hot Rock Projectile Kilauea in Hawaii is the world’s most continuously active volcano. Very active volcanoes characteristically eject red-hot rocks and lava rather than smoke and ash. Suppose a large rock is ejected from the volcano with a speed of 25.0 m/s and at an angle 35.0º above the horizontal, as shown in Figure 3.40. The rock strikes the side of the volcano at an altitude 20.0 m lower than its starting point. (a) Calculate the time it takes the rock to follow this path. (b) What are the magnitude and direction of the rock’s velocity at impact?.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSe88SkJqY1b",
        "outputId": "861ffd1b-ccaf-46b4-d994-22143ca8956e"
      },
      "source": [
        "with open('/content/SciQ dataset-2 3/train.json') as f :\n",
        "    data = json.load(f)\n",
        "    #print(data[0])\n",
        "    count = 0\n",
        "    qanda = []\n",
        "    for para in data :\n",
        "        qa = para['question'] + '\\t' + para['correct_answer'] + '\\n'\n",
        "        \n",
        "        qanda.append(qa)\n",
        "    print(qanda[0:5])\n",
        "\n",
        "with open('/content/train_qa/QA.txt', 'w') as m :\n",
        "    m.writelines(qanda)\n",
        "\n",
        "with open('/content/train_qa/QA.tsv', 'w') as m :\n",
        "    m.writelines(qanda)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What type of organism is commonly used in preparation of foods such as cheese and yogurt?\\tmesophilic organisms\\n', 'What phenomenon makes global winds blow northeast to southwest or the reverse in the northern hemisphere and northwest to southeast or the reverse in the southern hemisphere?\\tcoriolis effect\\n', 'Changes from a less-ordered state to a more-ordered state (such as a liquid to a solid) are always what?\\texothermic\\n', 'What is the least dangerous radioactive decay?\\talpha decay\\n', 'Kilauea in hawaii is the world’s most continuously active volcano. very active volcanoes characteristically eject red-hot rocks and lava rather than this?\\tsmoke and ash\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl2t2RexsdKD"
      },
      "source": [
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQduluxh9VmX"
      },
      "source": [
        "##Testing ner\n",
        "import spacy\n",
        "import random\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "  \n",
        "sentence = \"What type of organism is commonly used in preparation of foods such as cheese and yogurt\"\n",
        "  \n",
        "doc = nlp(sentence)\n",
        "\n",
        "for token in doc:\n",
        "  print(token, token.pos_)\n",
        "    \n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lehv1trJqGN0",
        "outputId": "920f6959-fda1-4caa-bec9-fa6862027112"
      },
      "source": [
        "indices = list(range(1, 11677))\n",
        "test_indices = random.sample(indices, 3000)\n",
        "val_indices = random.sample(list(set(indices) - set(test_indices)), 2000)\n",
        "print(len(indices),len(test_indices),len(val_indices))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11676 3000 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb-ObeXW9bWD"
      },
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "def clean_text2(p):\n",
        "  numbers = \"[0-9]\"\n",
        "  special = \"\\W+\"\n",
        "  p = re.sub(numbers, \"\", p)\n",
        "  p = re.sub(special, \"\", p)\n",
        "\n",
        "  return p\n",
        "\n",
        "def Intersection(lst1, lst2):\n",
        "    return list(set(lst1).intersection(lst2))\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/Libkge/data/aristo/entities.txt') as f:\n",
        "  entity_list = f.readlines()\n",
        "  entity_list = list(map(lambda x: x.strip(\"\\n\"), entity_list))\n",
        "# print(entity_list[0:10])\n",
        "# print(\"type\" in entity_list)\n",
        "\n",
        "max_len = 0\n",
        "indices = list(range(1, 11677))\n",
        "# test_indices = random.sample(indices, 3000)\n",
        "val_indices = random.sample(indices, 2000)\n",
        "\n",
        "with open('/content/SciQ dataset-2 3/train.json') as f :\n",
        "    data = json.load(f)\n",
        "    #print(data[0])\n",
        "    count = 0\n",
        "    train_ans_in_entity = 0\n",
        "    count_train = 0\n",
        "    count_val = 0\n",
        "    count_test = 0\n",
        "    qanda_train = []\n",
        "    qanda_val = []\n",
        "    qanda_test = []\n",
        "    for para in tqdm(data) :\n",
        "\n",
        "        if para['correct_answer'] not in entity_list:\n",
        "              continue\n",
        "\n",
        "        sentence = para['question']\n",
        "        doc = nlp(sentence)\n",
        "\n",
        "        words = [token.lemma_ for token in doc]\n",
        "        # print('sentence')\n",
        "        # print(sentence)\n",
        "        # words = sentence.split(\" \")\n",
        "        words = list(map(clean_text2, words))\n",
        "        max_len = max(max_len, len(words))\n",
        "        # print('words')\n",
        "        # print(words)\n",
        "        topic_entities = Intersection(words, entity_list)\n",
        "        # print('topic_entities')\n",
        "        # print(topic_entities)\n",
        "\n",
        "        # print()\n",
        "        # print()\n",
        "        try:\n",
        "          topic_entity = topic_entities[0]\n",
        "\n",
        "          qa = para['question'] + f' [{topic_entity}]' + '\\t' + para['correct_answer'] + '\\n'\n",
        "\n",
        "\n",
        "          if count in val_indices:\n",
        "            count_val += 1        \n",
        "            qanda_val.append(qa)\n",
        "\n",
        "          # elif count in test_indices:\n",
        "          #   count_test += 1\n",
        "          #   qanda_test.append(qa)\n",
        "          \n",
        "          else:\n",
        "            count_train += 1\n",
        "            qanda_train.append(qa)\n",
        "\n",
        "        except Exception as E:\n",
        "          # print(f\"***{count}***\")\n",
        "          # print(E)\n",
        "          # print(sentence)\n",
        "          # print(topic_entities)\n",
        "          pass\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    print()\n",
        "    print(qanda_test[0:1])\n",
        "    print(count_train, count_val, count_test)\n",
        "\n",
        "# with open('/content/train_qa/QA.txt', 'w') as m :\n",
        "#     m.writelines(qanda)\n",
        "\n",
        "# with open('/content/train_qa/QA.tsv', 'w') as m :\n",
        "#     m.writelines(qanda)\n",
        "print(train_ans_in_entity) \n",
        "print(max_len)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-1XghDUqqRX"
      },
      "source": [
        "print(qanda_val[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEyFryZxk6PL"
      },
      "source": [
        "with open('/content/train_qa/QA_reduced.txt', 'w') as m :\n",
        "    m.writelines(qanda_train)\n",
        "\n",
        "with open('/content/train_qa/QA_val.txt', 'w') as m :\n",
        "    m.writelines(qanda_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTuxQee3sGCI"
      },
      "source": [
        "import os\n",
        "os.mkdir(\"/content/test_qa\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMstzQ5t-soR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "869c2cc6-a650-433e-b3b5-d502f081553d"
      },
      "source": [
        "with open('/content/SciQ dataset-2 3/test.json') as f :\n",
        "    data = json.load(f)\n",
        "    #print(data[0])\n",
        "    count = 0\n",
        "    qanda_test = []\n",
        "    ans_choices_test = []\n",
        "    questions = []\n",
        "    for para in tqdm(data) :\n",
        "        # print(para)\n",
        "        if para['correct_answer'] not in entity_list:\n",
        "              continue\n",
        "\n",
        "        sentence = para['question']\n",
        "        doc = nlp(sentence)\n",
        "\n",
        "        words = [token.lemma_ for token in doc]\n",
        "        # print('sentence')\n",
        "        # print(sentence)\n",
        "        # words = sentence.split(\" \")\n",
        "        words = list(map(clean_text2, words))\n",
        "        max_len = max(max_len, len(words))\n",
        "        # print('words')\n",
        "        # print(words)\n",
        "        topic_entities = Intersection(words, entity_list)\n",
        "        # print('topic_entities')\n",
        "        # print(topic_entities)\n",
        "\n",
        "        # print()\n",
        "        # print()\n",
        "        try:\n",
        "          topic_entity = topic_entities[0]\n",
        "\n",
        "          qa = para['question'] + f' [{topic_entity}]' + '\\t' + para['correct_answer'] + '\\n'\n",
        "          questions.append(para['question'] + f' [{topic_entity}]' + \"\\n\")\n",
        "          # print(qa)\n",
        "          choices = para['distractor1'] + '\\t' + para['distractor2'] + '\\t' + para['distractor3'] + '\\t' + para['correct_answer'] + '\\n'\n",
        "          # if count in val_indices:\n",
        "          #   count_val += 1        \n",
        "          #   qanda_val.append(qa)\n",
        "\n",
        "          # elif count in test_indices:\n",
        "          #   count_test += 1\n",
        "          #   qanda_test.append(qa)\n",
        "          \n",
        "          # else:\n",
        "          #   count_train += 1\n",
        "          ans_choices_test.append(choices)\n",
        "          qanda_test.append(qa)\n",
        "\n",
        "        except Exception as E:\n",
        "          # print(f\"***{count}***\")\n",
        "          # print(E)\n",
        "          # print(sentence)\n",
        "          # print(topic_entities)\n",
        "          pass\n",
        "\n",
        "        count += 1\n",
        "    # print(qanda_test[0:1])\n",
        "    # print(len(qanda_test))\n",
        "    print(para)\n",
        "with open('/content/test_qa/QA_test.txt', 'w') as m :\n",
        "    m.writelines(qanda_test)\n",
        "\n",
        "with open('/content/test_qa/questions.txt', 'w') as m :\n",
        "    m.writelines(questions)\n",
        "\n",
        "with open('/content/test_qa/QA_test.tsv', 'w') as m :\n",
        "    m.writelines(qanda_test)\n",
        "\n",
        "with open('/content/test_qa/QA_choices.txt', 'w') as m :\n",
        "    m.writelines(ans_choices_test)\n",
        "    print(ans_choices_test[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:02<00:00, 379.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': 'In what form is atmospheric sulfur found?', 'distractor3': 'sulfur monoxide', 'distractor1': 'formaldehyde', 'distractor2': 'sulfuric acid', 'correct_answer': 'sulfur dioxide (so2)', 'support': 'On land, sulfur is deposited in four major ways: precipitation, direct fallout from the atmosphere, rock weathering, and geothermal vents (Figure 20.17). Atmospheric sulfur is found in the form of sulfur dioxide (SO2), and as rain falls through the atmosphere, sulfur is dissolved in the form of weak sulfuric acid (H2SO4). Sulfur can also fall directly from the atmosphere in a process called fallout. Also, as sulfur-containing rocks weather, sulfur is released into the soil. These rocks originate from ocean sediments that are moved to land by the geologic uplifting of ocean sediments. Terrestrial ecosystems can then make use of these soil sulfates (SO42-), which enter the food web by being taken up by plant roots. When these plants decompose and die, sulfur is released back into the atmosphere as hydrogen sulfide (H2S) gas.'}\n",
            "Bones\tMuscles\tThumbs\tbackbone\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYFGyVBFPDHT"
      },
      "source": [
        "!cp /content/test_qa/QA_test.txt /content/drive/MyDrive/EmbedKGQA/data/QA_data/QA_test.txt\n",
        "!cp /content/test_qa/QA_choices.txt /content/drive/MyDrive/EmbedKGQA/data/QA_data/QA_choices.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "776yCe-mv-67"
      },
      "source": [
        "distractors = []\n",
        "ans_not_in_entity = 0\n",
        "correct_not_in_entity = 0\n",
        "correct_in_entity = 0\n",
        "with open('/content/drive/MyDrive/EmbedKGQA/data/QA_choices.txt') as f:  \n",
        "  choices = f.readlines()\n",
        "  # print(choices)\n",
        "  for choice in choices:\n",
        "    choice = choice.strip(\"\\n\").split('\\t')\n",
        "    ans_string = f\"{choice[0]} {choice[1]} {choice[2]} {choice[3]}\"\n",
        "    sentence = ans_string\n",
        "    doc = nlp(sentence)\n",
        "    words = [token.lemma_ for token in doc]\n",
        "    # print(words)\n",
        "    # print(entity_list[:10])\n",
        "    if words[3] not in entity_list:\n",
        "      continue\n",
        "\n",
        "    # distractors.append([choice[0], choice[1], choice[2], choice[3]])\n",
        "print(ans_not_in_entity)\n",
        "print(correct_in_entity)\n",
        "print(correct_not_in_entity)\n",
        "# print(distractors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahAtO-R04oxi"
      },
      "source": [
        "[], []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE2b1swShNlS"
      },
      "source": [
        "def get_questions_with_ent(questionA, entity_list):\n",
        "  removed_tags = ['ADJ', 'DET', 'PRON', 'CONJ', 'CCONJ', 'AUX', 'ADP', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'PART', 'ADV']\n",
        "  # print(questionA)\n",
        "  question = questionA.split('[')[0]\n",
        "  ans = questionA.split('\\t')[-1]\n",
        "  doc = nlp(question)\n",
        "  # print(sentence)\n",
        "  words = [token.lemma_ for token in doc]\n",
        "  # print('sentence')\n",
        "  # print(sentence)\n",
        "  words = clean_text2(list2sent(words))\n",
        "  # print(words)\n",
        "  topic_entities = Intersection_substr(words, entity_list)\n",
        "  # print(topic_entities)\n",
        "  topic_sent = list2sent(topic_entities)\n",
        "\n",
        "  # doc = nlp(topic_sent)\n",
        "  # # topic_entities = [(str(entity), entity.pos_ ) for entity in doc]\n",
        "  # # print(topic_entities)\n",
        "  # topic_entities = [str(entity) for entity in doc if entity.pos_ not in removed_tags]\n",
        "  # topic_entities = list(set(topic_entities))\n",
        "\n",
        "  #multi word entities\n",
        "  tes = []\n",
        "  for te in topic_entities:\n",
        "    if \" \" in te:\n",
        "      doc = nlp(te)\n",
        "      temp = [(str(entity), entity.pos_) for entity in doc if entity.pos_ not in removed_tags]\n",
        "      # print(temp)\n",
        "      if temp:\n",
        "        tes.append(te)\n",
        "        # Flag = True\n",
        "  # print(list(set(topic_entities) - set(tes)))\n",
        "  topic_sent = list2sent(list(set(topic_entities) - set(tes)))\n",
        "\n",
        "  doc = nlp(topic_sent)\n",
        "  te = [str(entity) for entity in doc if entity.pos_ not in removed_tags]\n",
        "  te = list(set(te))\n",
        "  \n",
        "  tes = tes + te\n",
        "\n",
        "  topic_entities = Intersection(topic_entities, tes)\n",
        "\n",
        "  ques_template = f\"{question} [te]\"\n",
        "  questions = []\n",
        "  for te in topic_entities:\n",
        "    # if te == 'metropolitan area':\n",
        "    #   print(topic_entities)\n",
        "    #   print(ques_template)\n",
        "    questions.append(ques_template.replace('[te]', f'[{te}]'))\n",
        "\n",
        "  return questions, ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhVU8jKf7rRG"
      },
      "source": [
        "def get_ans(questions, ans):\n",
        "\n",
        "  for ques in questions:\n",
        "    temp_ans = get_prediction(model, ques, e, entity2idx, idx2entity, embedding_matrix)\n",
        "    if temp_ans == ans:\n",
        "      return True\n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYP4hUUV7s7d"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from dataloader import DatasetMetaQA, DataLoaderMetaQA\n",
        "from model import RelationExtractor\n",
        "import networkx as nx\n",
        "from kge.model import KgeModel\n",
        "from kge.util.io import load_checkpoint\n",
        "\n",
        "def getEntityEmbeddings(kge_model):\n",
        "    e = {}\n",
        "    entity_dict = f'../../pretrained_models/embeddings/commonsense/entity_ids.del'\n",
        "    embedder = kge_model._entity_embedder\n",
        "    f = open(entity_dict, 'r')\n",
        "    for line in f:\n",
        "        line = line[:-1].split('\\t')\n",
        "        ent_id = int(line[0])\n",
        "        ent_name = line[1]\n",
        "        e[ent_name] = embedder._embeddings(torch.LongTensor([ent_id]))[0]\n",
        "    f.close()\n",
        "    return e\n",
        "\n",
        "def prepare_embeddings(embedding_dict):\n",
        "    entity2idx = {}\n",
        "    idx2entity = {}\n",
        "    i = 0\n",
        "    embedding_matrix = []\n",
        "    for key, entity in embedding_dict.items():\n",
        "        entity2idx[key] = i\n",
        "        idx2entity[i] = key\n",
        "        i += 1\n",
        "        embedding_matrix.append(entity)\n",
        "    return entity2idx, idx2entity, embedding_matrix\n",
        "\n",
        "\n",
        "embedding_dim = 256\n",
        "relation_dim = 50\n",
        "model_name = 'ComplEx'\n",
        "do_batch_norm = 1\n",
        "\n",
        "def load_models(load_from):\n",
        "    \n",
        "    #Load KG Embeddings\n",
        "    checkpoint_file = f'../../pretrained_models/embeddings/{load_from}/checkpoint_best.pt'\n",
        "    print('Loading kg embeddings from', checkpoint_file)\n",
        "    kge_checkpoint = load_checkpoint(checkpoint_file)\n",
        "    kge_model = KgeModel.create_from(kge_checkpoint)\n",
        "    kge_model.eval()\n",
        "    e = getEntityEmbeddings(kge_model)\n",
        "\n",
        "    print('Loaded entities and relations')\n",
        "\n",
        "    entity2idx, idx2entity, embedding_matrix = prepare_embeddings(e)\n",
        "\n",
        "\n",
        "    #Load RE Model\n",
        "    model = RelationExtractor(embedding_dim=embedding_dim, num_entities = len(idx2entity), relation_dim=relation_dim, \n",
        "                              pretrained_embeddings=embedding_matrix, device=device, \n",
        "                              model = model_name, do_batch_norm=do_batch_norm)\n",
        "      \n",
        "    fname = \"/content/drive/MyDrive/EmbedKGQA_final/output/\" + load_from + \".pt\"\n",
        "    print('Loading from %s' % fname)\n",
        "    model.load_state_dict(torch.load(fname, map_location=torch.device('cpu')))\n",
        "    print('Loaded successfully!')\n",
        "    model.to(device)\n",
        "\n",
        "    return model, e, entity2idx, idx2entity, embedding_matrix\n",
        "\n",
        "def get_prediction(model, question, e, entity2idx, idx2entity, embedding_matrix):\n",
        "\n",
        "  #Getting head and question\n",
        "  # print(question)\n",
        "  question_t = question.split('\\t')[0]\n",
        "  # print(question)\n",
        "  question = question.split('[')\n",
        "  question_1 = question[0]\n",
        "  question_2 = question[1].split(']')\n",
        "  head = question_2[0].strip()\n",
        "  question_2 = question_2[1]\n",
        "  question = question_1+'NE'+question_2\n",
        "  # print(question, head)\n",
        "  head = entity2idx[head.strip()]\n",
        "  head = torch.tensor(head, dtype=torch.long)\n",
        "  head = head.to(device)\n",
        "  data = [head, question]\n",
        "  # Get_prediction\n",
        "  dataloader = DatasetMetaQA(data, e, entity2idx)\n",
        "  \n",
        "  question_tokenized, attention_mask = dataloader.tokenize_question(question)\n",
        "  question_tokenized = question_tokenized.to(device)\n",
        "  attention_mask = attention_mask.to(device)\n",
        "\n",
        "  # print(head)\n",
        "  scores = model.get_score_ranked(head=head, question_tokenized=question_tokenized, attention_mask=attention_mask)[0]\n",
        "  mask = torch.zeros(len(entity2idx)).to(device)\n",
        "  mask[head] = 1\n",
        "  #reduce scores of all non-candidates\n",
        "  new_scores = scores - (mask*99999)\n",
        "  pred_ans = torch.argmax(new_scores).item()\n",
        "\n",
        "  return idx2entity[pred_ans]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-a_Kf9E7vNH"
      },
      "source": [
        "# model, e, entity2idx, idx2entity, embedding_matrix = load_models('aristo_final')\n",
        "from tqdm import tqdm\n",
        "\n",
        "quesAns = get_questions()[0]\n",
        "correct = 0\n",
        "total = 0\n",
        "for questionA in tqdm(quesAns):\n",
        "  questions, ans = get_questions_with_ent(questionA, entity_list)\n",
        "  # print(questions)\n",
        "  if get_ans(questions, ans):\n",
        "    correct += 1\n",
        "  total += 1\n",
        "  if total % 50 == 0:\n",
        "    print(correct)\n",
        "  # print(questions)\n",
        "  # print(ans)\n",
        "  # print(\"**next**\")\n",
        "print(f\"{correct/total}%\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}